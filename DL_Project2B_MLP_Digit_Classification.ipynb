{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5XzVh-J0-fu"
   },
   "source": [
    "# MNIST MLP Digit Recognition Network\n",
    "\n",
    "We will code a basic digit recognition network. The data are images which specify the digits 1 to 10 as (1, 28, 28) data - this data is black and white images. Each pixed of the image is an intensity between 0 and 255, and together the (1, 28, 28) pixel image can be visualized as a picture of a digit. The data is given to you as $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where $y$ is the given label and x is the (1, 28, 28) data. This data will be gotten from `torchvision`, a repository of computer vision data and models.\n",
    "\n",
    "Highlevel, the model and notebook goes as follows:\n",
    "*   You first download the data and specify the batch size of B = 16. Each image will need to be turned from a (1, 28, 28) volume into a vector of dimension 784 = 1 * 28 * 28. So each batch will be of size (16, 784).\n",
    "*   Then, you pass the model through two hidden layers, one of dimension (784, 32) and another of dimension (32, 16). After each linear map, you pass the data through a TanH nonlinearity.\n",
    "*   Finally, you pass the data through a (32, 10) linear layer and you return the log softmax of the data.\n",
    "*   What objective do you use? Be careful!\n",
    "*   How do you compute accuracy both manually and with torchmetrics?\n",
    "*   How do you compute AUROC?\n",
    "\n",
    "All asserts should pass and accuracy should be higher than 85%. Otheer nonlinearity, like ReLU, might get higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfKuo0JFxJ7d",
    "outputId": "d1a71e6e-5dd3-479e-9b92-4ac48121a61d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/shijia_huang/.local/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: packaging in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (20.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from packaging->torchmetrics) (2.4.6)\n",
      "Requirement already satisfied: six in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from packaging->torchmetrics) (1.14.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8UDIb4ldyj2C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdLoOr08AUY5",
    "outputId": "6747b7e1-20ae-4c4c-b0f8-51eb0a3a64d8"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jLD0oQmgxxlR"
   },
   "outputs": [],
   "source": [
    "image_path = './'\n",
    "\n",
    "# Use ToTensor to transform the data and scale it by 255\n",
    "# Look up transforms and Compose as well\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    "  )\n",
    "\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xGLuLaEXyzoD"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "# Define the DL for train and test\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(mnist_test_dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PjLznvm8xqaT"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Define the layers\n",
    "    self.linear1 = nn.Linear(784, 32)\n",
    "    self.linear2 = nn.Linear(32, 16)\n",
    "    self.linear3 = nn.Linear(16, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Flatten x to be of last dimension 784\n",
    "    x = x.view(x.size(0),-1)\n",
    "\n",
    "    # Pass through linear layer 1\n",
    "    x = self.linear1(x)\n",
    "\n",
    "    # Apply tanh\n",
    "    x = torch.tanh(x)\n",
    "\n",
    "    # Pass through linear layer 2\n",
    "    x = self.linear2(x)\n",
    "\n",
    "    # Apply tanh\n",
    "    x = torch.tanh(x)\n",
    "\n",
    "    # Pass through linear layer 3\n",
    "    x = self.linear3(x)\n",
    "\n",
    "    # Return the LogSoftmax of the data\n",
    "    # This will affect the loss we choose below\n",
    "    return nn.LogSoftmax(dim = 1)(x)\n",
    "\n",
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zu4Z0aptxqcu",
    "outputId": "0175645e-6def-438b-8a7a-8c420ec67844",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics Epoch 0 Loss 0.0354 Accuracy 0.2251 AUROC 0.7630\n",
      "Test Metrics Epoch 0 Loss 0.0348 Accuracy 0.3305 AUROC 0.8787\n",
      "Train Metrics Epoch 1 Loss 0.0339 Accuracy 0.4055 AUROC 0.8960\n",
      "Test Metrics Epoch 1 Loss 0.0331 Accuracy 0.5435 AUROC 0.9134\n",
      "Train Metrics Epoch 2 Loss 0.0320 Accuracy 0.5687 AUROC 0.9187\n",
      "Test Metrics Epoch 2 Loss 0.0310 Accuracy 0.6048 AUROC 0.9312\n",
      "Train Metrics Epoch 3 Loss 0.0298 Accuracy 0.6139 AUROC 0.9334\n",
      "Test Metrics Epoch 3 Loss 0.0286 Accuracy 0.6390 AUROC 0.9444\n",
      "Train Metrics Epoch 4 Loss 0.0273 Accuracy 0.6353 AUROC 0.9408\n",
      "Test Metrics Epoch 4 Loss 0.0261 Accuracy 0.6526 AUROC 0.9492\n",
      "Train Metrics Epoch 5 Loss 0.0249 Accuracy 0.6420 AUROC 0.9476\n",
      "Test Metrics Epoch 5 Loss 0.0238 Accuracy 0.6528 AUROC 0.9556\n",
      "Train Metrics Epoch 6 Loss 0.0228 Accuracy 0.6487 AUROC 0.9536\n",
      "Test Metrics Epoch 6 Loss 0.0218 Accuracy 0.6642 AUROC 0.9588\n",
      "Train Metrics Epoch 7 Loss 0.0209 Accuracy 0.6676 AUROC 0.9585\n",
      "Test Metrics Epoch 7 Loss 0.0200 Accuracy 0.6939 AUROC 0.9626\n",
      "Train Metrics Epoch 8 Loss 0.0193 Accuracy 0.6987 AUROC 0.9626\n",
      "Test Metrics Epoch 8 Loss 0.0185 Accuracy 0.7282 AUROC 0.9674\n",
      "Train Metrics Epoch 9 Loss 0.0178 Accuracy 0.7330 AUROC 0.9663\n",
      "Test Metrics Epoch 9 Loss 0.0171 Accuracy 0.7586 AUROC 0.9729\n",
      "Train Metrics Epoch 10 Loss 0.0166 Accuracy 0.7625 AUROC 0.9686\n",
      "Test Metrics Epoch 10 Loss 0.0159 Accuracy 0.7843 AUROC 0.9746\n",
      "Train Metrics Epoch 11 Loss 0.0154 Accuracy 0.7849 AUROC 0.9723\n",
      "Test Metrics Epoch 11 Loss 0.0148 Accuracy 0.8056 AUROC 0.9761\n",
      "Train Metrics Epoch 12 Loss 0.0145 Accuracy 0.8023 AUROC 0.9739\n",
      "Test Metrics Epoch 12 Loss 0.0138 Accuracy 0.8203 AUROC 0.9779\n",
      "Train Metrics Epoch 13 Loss 0.0136 Accuracy 0.8163 AUROC 0.9751\n",
      "Test Metrics Epoch 13 Loss 0.0130 Accuracy 0.8280 AUROC 0.9802\n",
      "Train Metrics Epoch 14 Loss 0.0128 Accuracy 0.8257 AUROC 0.9770\n",
      "Test Metrics Epoch 14 Loss 0.0123 Accuracy 0.8357 AUROC 0.9811\n",
      "Train Metrics Epoch 15 Loss 0.0121 Accuracy 0.8335 AUROC 0.9776\n",
      "Test Metrics Epoch 15 Loss 0.0117 Accuracy 0.8415 AUROC 0.9836\n",
      "Train Metrics Epoch 16 Loss 0.0115 Accuracy 0.8400 AUROC 0.9793\n",
      "Test Metrics Epoch 16 Loss 0.0111 Accuracy 0.8463 AUROC 0.9845\n",
      "Train Metrics Epoch 17 Loss 0.0110 Accuracy 0.8445 AUROC 0.9797\n",
      "Test Metrics Epoch 17 Loss 0.0106 Accuracy 0.8507 AUROC 0.9870\n",
      "Train Metrics Epoch 18 Loss 0.0106 Accuracy 0.8488 AUROC 0.9810\n",
      "Test Metrics Epoch 18 Loss 0.0102 Accuracy 0.8550 AUROC 0.9854\n",
      "Train Metrics Epoch 19 Loss 0.0101 Accuracy 0.8534 AUROC 0.9813\n",
      "Test Metrics Epoch 19 Loss 0.0098 Accuracy 0.8593 AUROC 0.9860\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import Accuracy,AUROC ##\n",
    "\n",
    "# Get the loss function; remember you are outputting the LogSoftmax so be careful what loss you pick\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# Set the optimizer to SGD and let the learning rate be LR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "for epoch in range(EPOCHS):\n",
    "    accuracy_hist_train = 0\n",
    "    auroc_hist_train = 0.0\n",
    "    loss_hist_train = 0\n",
    "    # Loop through the x and y pairs of data\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # Get the model predictions\n",
    "        pred = model(x_batch) \n",
    "\n",
    "        # Get the loss\n",
    "        loss = loss_fn(pred,y_batch)\n",
    "\n",
    "        # Get the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Add to the loss\n",
    "        # Note that loss is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
    "        loss_hist_train += loss.item()\n",
    "\n",
    "        # Update the prameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the number of correct predictions, \n",
    "        # This should be a tensor\n",
    "        is_correct_1 = (torch.argmax(pred, dim = 1) == y_batch).float()\n",
    "\n",
    "        # Get the number of correct predictions, do this with torchmetrics\n",
    "        # This should be a Float\n",
    "        is_correct_2 = Accuracy(task='multiclass',num_classes=10)(pred.argmax(dim=1), y_batch) * len(y_batch)\n",
    "\n",
    "        assert(is_correct_1.sum() == is_correct_2)\n",
    "\n",
    "        accuracy_hist_train += is_correct_1.sum() \n",
    "\n",
    "        # Get the AUROC - make sure to multiply by the batch length since this is just the AUC over the batch and you want to take a weighted average later\n",
    "        auroc_hist_train += AUROC(task=\"multiclass\", num_classes=10)(pred,y_batch) * BATCH_SIZE\n",
    "        \n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    auroc_hist_train /= len(train_dl.dataset)\n",
    "    loss_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Train Metrics Epoch {epoch} Loss {loss_hist_train:.4f} Accuracy {accuracy_hist_train:.4f} AUROC {auroc_hist_train:.4f}')\n",
    "\n",
    "    accuracy_hist_test = 0\n",
    "    auroc_hist_test = 0.0\n",
    "    loss_hist_test = 0.0\n",
    "    # Get the average value of each metric across the test batches\n",
    "    # Add a \"with\" clause here so that no gradients are computed; we want to just evaluate the model\n",
    "    with torch.no_grad():\n",
    "      accuracy_hist_test = 0\n",
    "      auroc_hist_test = 0.0\n",
    "      # Loop through the x and y pairs of data\n",
    "      for x_batch, y_batch in test_dl:\n",
    "          # Get he the model predictions\n",
    "          pred = model(x_batch) ##\n",
    "\n",
    "          # Get the loss\n",
    "          loss = loss_fn(pred, y_batch)\n",
    "\n",
    "          # Add to the loss\n",
    "          # Note that loss is a mean over the batch size and we need the total sum over the number of samples in the dataset\n",
    "          loss_hist_test += loss.item()\n",
    "\n",
    "          # Get the number of correct predictions via torchmetrics\n",
    "          is_correct = Accuracy(task='multiclass',num_classes=10)(pred.argmax(dim=1), y_batch)* len(y_batch)\n",
    "\n",
    "          # Get the accuracy\n",
    "          accuracy_hist_test += is_correct\n",
    "\n",
    "          # Get AUROC\n",
    "          auroc_hist_test += AUROC(task=\"multiclass\", num_classes=10)(pred,y_batch) * BATCH_SIZE\n",
    "      # Normalize the metrics by the right number\n",
    "      accuracy_hist_test /= len(test_dl.dataset)\n",
    "      auroc_hist_test /= len(test_dl.dataset)\n",
    "      loss_hist_test /= len(test_dl.dataset)\n",
    "      print(f'Test Metrics Epoch {epoch} Loss {loss_hist_test:.4f} Accuracy {accuracy_hist_test:.4f} AUROC {auroc_hist_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Final Test accuracy: 0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Final Test accuracy: 0.8593\n"
     ]
    }
   ],
   "source": [
    "# Get train/test final accuracy directly; normalize the data by 255.0\n",
    "# Should be around 85%\n",
    "\n",
    "train_x = torch.stack([torch.tensor(tup[0]) for tup in mnist_train_dataset])\n",
    "train_y = torch.stack([torch.tensor(tup[1]) for tup in mnist_train_dataset])\n",
    "\n",
    "pred = model((train_x))\n",
    "is_correct = (torch.argmax(pred, dim = 1) == train_y).float()\n",
    "print(f'Total Final Test accuracy: {is_correct.mean():.4f}')\n",
    "\n",
    "test_x = torch.stack([torch.tensor(tup[0]) for tup in mnist_test_dataset])\n",
    "test_y = torch.stack([torch.tensor(tup[1]) for tup in mnist_test_dataset])\n",
    "\n",
    "pred = model((test_x))\n",
    "is_correct = (torch.argmax(pred, dim = 1) == test_y).float()\n",
    "print(f'Total Final Test accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
